{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26adef78",
   "metadata": {
    "id": "26adef78"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc8b0718",
   "metadata": {
    "id": "dc8b0718"
   },
   "outputs": [],
   "source": [
    "def scrape_page(page_number):\n",
    "    url = f\"https://www.trustpilot.com/review/www.fashionnova.com?page={page_number}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    reviews = []\n",
    "\n",
    "    # Find all review containers\n",
    "    review_divs = soup.find_all('div', class_='styles_reviewCardInner__EwDq2')\n",
    "\n",
    "    for review_div in review_divs:\n",
    "        # Extract reviewer name\n",
    "        reviewer_name_tag = review_div.find('span', class_='typography_heading-xxs__QKBS8', attrs={\"data-consumer-name-typography\": \"true\"})\n",
    "        reviewer_name = reviewer_name_tag.get_text(strip=True) if reviewer_name_tag else \"Name not found\"\n",
    "\n",
    "        # Extract reviewer profile link\n",
    "        reviewer_profile_link_tag = review_div.find('a', class_='link_internal__7XN06', attrs={\"data-consumer-profile-link\": \"true\"})\n",
    "        reviewer_profile_link = reviewer_profile_link_tag['href'] if reviewer_profile_link_tag else \"Profile link not found\"\n",
    "\n",
    "        # Extract reviewer country\n",
    "        reviewer_country_tag = review_div.find('div', class_='typography_body-m__xgxZ_ typography_appearance-subtle__8_H2l styles_detailsIcon__Fo_ua')\n",
    "        reviewer_country = reviewer_country_tag.find('span').text if reviewer_country_tag else \"Country not found\"\n",
    "\n",
    "        # Extract review count\n",
    "        review_count_tag = review_div.find('span', class_='typography_body-m__xgxZ_ typography_appearance-subtle__8_H2l', attrs={'data-consumer-reviews-count-typography': 'true'})\n",
    "        review_count = review_count_tag.text.strip() if review_count_tag else \"Review count not found\"\n",
    "\n",
    "        # Extract review date\n",
    "        review_date_tag = review_div.find('time', attrs={'data-service-review-date-time-ago': 'true'})\n",
    "        review_date = review_date_tag['datetime'] if review_date_tag else \"Review date not found\"\n",
    "\n",
    "        # Extract rating\n",
    "        rating_tag = review_div.find('div', class_='star-rating_starRating__4rrcf')\n",
    "        rating = rating_tag.find('img')['alt'] if rating_tag else \"Rating not found\"\n",
    "\n",
    "        # Extract review title\n",
    "        review_title_tag = review_div.find('h2', class_='typography_heading-s__f7029')\n",
    "        review_title = review_title_tag.text if review_title_tag else \"Review title not found\"\n",
    "\n",
    "        # Extract review text\n",
    "        review_text_tag = review_div.find('p', class_='typography_body-l__KUYFJ typography_appearance-default__AAY17 typography_color-black__5LYEn')\n",
    "        review_text = review_text_tag.text if review_text_tag else \"Review text not found\"\n",
    "\n",
    "        # Extract date of experience\n",
    "        date_experience_tag = review_div.find('p', class_='typography_body-m__xgxZ_ typography_appearance-default__AAY17', attrs={'data-service-review-date-of-experience-typography': 'true'})\n",
    "        date_experience = date_experience_tag.get_text(strip=True).split(':', 1)[-1].strip() if date_experience_tag else \"Date of experience not found\"\n",
    "\n",
    "        reviews.append({\n",
    "            \"Reviewer Name\": reviewer_name,\n",
    "            \"Profile Link\": reviewer_profile_link,\n",
    "            \"Country\": reviewer_country,\n",
    "            \"Review Count\": review_count,\n",
    "            \"Review Date\": review_date,\n",
    "            \"Rating\": rating,\n",
    "            \"Review Title\": review_title,\n",
    "            \"Review Text\": review_text,\n",
    "            \"Date of Experience\": date_experience\n",
    "        })\n",
    "\n",
    "    return reviews\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c_7To3i7tkNF",
   "metadata": {
    "id": "c_7To3i7tkNF"
   },
   "outputs": [],
   "source": [
    "def scrape_pages(start_page, end_page):\n",
    "    all_reviews = []\n",
    "    for page_number in range(start_page, end_page + 1):\n",
    "        reviews = scrape_page(page_number)\n",
    "        all_reviews.extend(reviews)\n",
    "\n",
    "        # Delay after every 10 pages\n",
    "        if page_number % 10 == 0:\n",
    "            print(f\"Scraped {page_number} pages. Waiting for 2 seconds...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "        # Delay after every 100 pages\n",
    "        if page_number % 100 == 0:\n",
    "            print(f\"Scraped {page_number} pages. Waiting for 5 minutes...\")\n",
    "            time.sleep(300)\n",
    "\n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d28f5a17-24bf-4d3d-9770-875950193e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 10 pages. Waiting for 2 seconds...\n",
      "Scraped 20 pages. Waiting for 2 seconds...\n",
      "Scraped 30 pages. Waiting for 2 seconds...\n",
      "Scraped 40 pages. Waiting for 2 seconds...\n",
      "Scraped 50 pages. Waiting for 2 seconds...\n",
      "Scraped 60 pages. Waiting for 2 seconds...\n",
      "Scraped 70 pages. Waiting for 2 seconds...\n",
      "Scraped 80 pages. Waiting for 2 seconds...\n",
      "Scraped 90 pages. Waiting for 2 seconds...\n",
      "Scraped 100 pages. Waiting for 2 seconds...\n",
      "Scraped 100 pages. Waiting for 5 minutes...\n",
      "Scraped 110 pages. Waiting for 2 seconds...\n",
      "Scraped 120 pages. Waiting for 2 seconds...\n",
      "Scraped 130 pages. Waiting for 2 seconds...\n",
      "Scraped 140 pages. Waiting for 2 seconds...\n",
      "Scraped 150 pages. Waiting for 2 seconds...\n",
      "Scraped 160 pages. Waiting for 2 seconds...\n",
      "Scraped 170 pages. Waiting for 2 seconds...\n",
      "Scraped 180 pages. Waiting for 2 seconds...\n",
      "Scraped 190 pages. Waiting for 2 seconds...\n",
      "Scraped 200 pages. Waiting for 2 seconds...\n",
      "Scraped 200 pages. Waiting for 5 minutes...\n",
      "Scraped 210 pages. Waiting for 2 seconds...\n",
      "Scraped 220 pages. Waiting for 2 seconds...\n",
      "Scraped 230 pages. Waiting for 2 seconds...\n",
      "Scraped 240 pages. Waiting for 2 seconds...\n",
      "Scraped 250 pages. Waiting for 2 seconds...\n",
      "Scraped 260 pages. Waiting for 2 seconds...\n",
      "Scraped 270 pages. Waiting for 2 seconds...\n",
      "Scraped 280 pages. Waiting for 2 seconds...\n",
      "Scraped 290 pages. Waiting for 2 seconds...\n",
      "Scraped 300 pages. Waiting for 2 seconds...\n",
      "Scraped 300 pages. Waiting for 5 minutes...\n",
      "Scraped 310 pages. Waiting for 2 seconds...\n",
      "Scraped 320 pages. Waiting for 2 seconds...\n",
      "Scraped 330 pages. Waiting for 2 seconds...\n",
      "Scraped 340 pages. Waiting for 2 seconds...\n",
      "Scraped 350 pages. Waiting for 2 seconds...\n",
      "Scraped 360 pages. Waiting for 2 seconds...\n",
      "Scraped 370 pages. Waiting for 2 seconds...\n",
      "Scraped 380 pages. Waiting for 2 seconds...\n",
      "Scraped 390 pages. Waiting for 2 seconds...\n",
      "Scraped 400 pages. Waiting for 2 seconds...\n",
      "Scraped 400 pages. Waiting for 5 minutes...\n",
      "Scraped 410 pages. Waiting for 2 seconds...\n",
      "Scraped 420 pages. Waiting for 2 seconds...\n",
      "Scraped 430 pages. Waiting for 2 seconds...\n",
      "Scraped 440 pages. Waiting for 2 seconds...\n",
      "Scraped 450 pages. Waiting for 2 seconds...\n",
      "Scraped 460 pages. Waiting for 2 seconds...\n",
      "Scraped 470 pages. Waiting for 2 seconds...\n",
      "Scraped 480 pages. Waiting for 2 seconds...\n",
      "Scraped 490 pages. Waiting for 2 seconds...\n",
      "Scraped 500 pages. Waiting for 2 seconds...\n",
      "Scraped 500 pages. Waiting for 5 minutes...\n",
      "Scraped 510 pages. Waiting for 2 seconds...\n",
      "Scraped 520 pages. Waiting for 2 seconds...\n",
      "Scraped 530 pages. Waiting for 2 seconds...\n",
      "Scraped 540 pages. Waiting for 2 seconds...\n",
      "Scraped 550 pages. Waiting for 2 seconds...\n",
      "Scraped 560 pages. Waiting for 2 seconds...\n",
      "Scraped 570 pages. Waiting for 2 seconds...\n",
      "Scraped 580 pages. Waiting for 2 seconds...\n",
      "Scraped 590 pages. Waiting for 2 seconds...\n",
      "Scraped 600 pages. Waiting for 2 seconds...\n",
      "Scraped 600 pages. Waiting for 5 minutes...\n",
      "Scraped 610 pages. Waiting for 2 seconds...\n",
      "Scraped 620 pages. Waiting for 2 seconds...\n",
      "Scraped 630 pages. Waiting for 2 seconds...\n",
      "Scraped 640 pages. Waiting for 2 seconds...\n",
      "Scraped 650 pages. Waiting for 2 seconds...\n",
      "Scraped 660 pages. Waiting for 2 seconds...\n",
      "Scraped 670 pages. Waiting for 2 seconds...\n",
      "Scraped 680 pages. Waiting for 2 seconds...\n",
      "Scraped 690 pages. Waiting for 2 seconds...\n",
      "Scraped 700 pages. Waiting for 2 seconds...\n",
      "Scraped 700 pages. Waiting for 5 minutes...\n",
      "Scraped 710 pages. Waiting for 2 seconds...\n",
      "Scraped 720 pages. Waiting for 2 seconds...\n",
      "Scraped 730 pages. Waiting for 2 seconds...\n",
      "Scraped 740 pages. Waiting for 2 seconds...\n",
      "Scraped 750 pages. Waiting for 2 seconds...\n",
      "Scraped 760 pages. Waiting for 2 seconds...\n",
      "Scraped 770 pages. Waiting for 2 seconds...\n",
      "Scraped 780 pages. Waiting for 2 seconds...\n",
      "Scraped 790 pages. Waiting for 2 seconds...\n",
      "Scraped 800 pages. Waiting for 2 seconds...\n",
      "Scraped 800 pages. Waiting for 5 minutes...\n",
      "Scraped 810 pages. Waiting for 2 seconds...\n",
      "Scraped 820 pages. Waiting for 2 seconds...\n",
      "Scraped 830 pages. Waiting for 2 seconds...\n",
      "Scraped 840 pages. Waiting for 2 seconds...\n",
      "Scraped 850 pages. Waiting for 2 seconds...\n",
      "Scraped 860 pages. Waiting for 2 seconds...\n",
      "Scraped 870 pages. Waiting for 2 seconds...\n",
      "Scraped 880 pages. Waiting for 2 seconds...\n",
      "Scraped 890 pages. Waiting for 2 seconds...\n",
      "Scraped 900 pages. Waiting for 2 seconds...\n",
      "Scraped 900 pages. Waiting for 5 minutes...\n",
      "Scraped 910 pages. Waiting for 2 seconds...\n",
      "Scraped 920 pages. Waiting for 2 seconds...\n",
      "Scraped 930 pages. Waiting for 2 seconds...\n",
      "Scraped 940 pages. Waiting for 2 seconds...\n",
      "Scraped 950 pages. Waiting for 2 seconds...\n",
      "Scraped 960 pages. Waiting for 2 seconds...\n",
      "Scraped 970 pages. Waiting for 2 seconds...\n",
      "Scraped 980 pages. Waiting for 2 seconds...\n",
      "Scraped 990 pages. Waiting for 2 seconds...\n",
      "Scraped 1000 pages. Waiting for 2 seconds...\n",
      "Scraped 1000 pages. Waiting for 5 minutes...\n"
     ]
    }
   ],
   "source": [
    "start_page = 1\n",
    "end_page = 1000\n",
    "frame1_reviews = scrape_pages(start_page, end_page)\n",
    "df1 = pd.DataFrame(frame1_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49bfb0dc-3ff5-4385-93ab-ace4d64ac239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 9)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dd547ed-6bd2-44d8-ae87-524fd0bda9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('frame1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aFI3dd8guDre",
   "metadata": {
    "id": "aFI3dd8guDre"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 1010 pages. Waiting for 2 seconds...\n",
      "Scraped 1020 pages. Waiting for 2 seconds...\n",
      "Scraped 1030 pages. Waiting for 2 seconds...\n",
      "Scraped 1040 pages. Waiting for 2 seconds...\n",
      "Scraped 1050 pages. Waiting for 2 seconds...\n",
      "Scraped 1060 pages. Waiting for 2 seconds...\n",
      "Scraped 1070 pages. Waiting for 2 seconds...\n",
      "Scraped 1080 pages. Waiting for 2 seconds...\n",
      "Scraped 1090 pages. Waiting for 2 seconds...\n",
      "Scraped 1100 pages. Waiting for 2 seconds...\n",
      "Scraped 1100 pages. Waiting for 5 minutes...\n",
      "Scraped 1110 pages. Waiting for 2 seconds...\n",
      "Scraped 1120 pages. Waiting for 2 seconds...\n",
      "Scraped 1130 pages. Waiting for 2 seconds...\n",
      "Scraped 1140 pages. Waiting for 2 seconds...\n",
      "Scraped 1150 pages. Waiting for 2 seconds...\n",
      "Scraped 1160 pages. Waiting for 2 seconds...\n",
      "Scraped 1170 pages. Waiting for 2 seconds...\n",
      "Scraped 1180 pages. Waiting for 2 seconds...\n",
      "Scraped 1190 pages. Waiting for 2 seconds...\n",
      "Scraped 1200 pages. Waiting for 2 seconds...\n",
      "Scraped 1200 pages. Waiting for 5 minutes...\n",
      "Scraped 1210 pages. Waiting for 2 seconds...\n",
      "Scraped 1220 pages. Waiting for 2 seconds...\n",
      "Scraped 1230 pages. Waiting for 2 seconds...\n",
      "Scraped 1240 pages. Waiting for 2 seconds...\n",
      "Scraped 1250 pages. Waiting for 2 seconds...\n",
      "Scraped 1260 pages. Waiting for 2 seconds...\n",
      "Scraped 1270 pages. Waiting for 2 seconds...\n",
      "Scraped 1280 pages. Waiting for 2 seconds...\n",
      "Scraped 1290 pages. Waiting for 2 seconds...\n",
      "Scraped 1300 pages. Waiting for 2 seconds...\n",
      "Scraped 1300 pages. Waiting for 5 minutes...\n",
      "Scraped 1310 pages. Waiting for 2 seconds...\n",
      "Scraped 1320 pages. Waiting for 2 seconds...\n",
      "Scraped 1330 pages. Waiting for 2 seconds...\n",
      "Scraped 1340 pages. Waiting for 2 seconds...\n",
      "Scraped 1350 pages. Waiting for 2 seconds...\n",
      "Scraped 1360 pages. Waiting for 2 seconds...\n",
      "Scraped 1370 pages. Waiting for 2 seconds...\n",
      "Scraped 1380 pages. Waiting for 2 seconds...\n",
      "Scraped 1390 pages. Waiting for 2 seconds...\n",
      "Scraped 1400 pages. Waiting for 2 seconds...\n",
      "Scraped 1400 pages. Waiting for 5 minutes...\n",
      "Scraped 1410 pages. Waiting for 2 seconds...\n",
      "Scraped 1420 pages. Waiting for 2 seconds...\n",
      "Scraped 1430 pages. Waiting for 2 seconds...\n",
      "Scraped 1440 pages. Waiting for 2 seconds...\n",
      "Scraped 1450 pages. Waiting for 2 seconds...\n",
      "Scraped 1460 pages. Waiting for 2 seconds...\n",
      "Scraped 1470 pages. Waiting for 2 seconds...\n",
      "Scraped 1480 pages. Waiting for 2 seconds...\n",
      "Scraped 1490 pages. Waiting for 2 seconds...\n",
      "Scraped 1500 pages. Waiting for 2 seconds...\n",
      "Scraped 1500 pages. Waiting for 5 minutes...\n",
      "Scraped 1510 pages. Waiting for 2 seconds...\n",
      "Scraped 1520 pages. Waiting for 2 seconds...\n",
      "Scraped 1530 pages. Waiting for 2 seconds...\n",
      "Scraped 1540 pages. Waiting for 2 seconds...\n",
      "Scraped 1550 pages. Waiting for 2 seconds...\n",
      "Scraped 1560 pages. Waiting for 2 seconds...\n",
      "Scraped 1570 pages. Waiting for 2 seconds...\n",
      "Scraped 1580 pages. Waiting for 2 seconds...\n",
      "Scraped 1590 pages. Waiting for 2 seconds...\n",
      "Scraped 1600 pages. Waiting for 2 seconds...\n",
      "Scraped 1600 pages. Waiting for 5 minutes...\n",
      "Scraped 1610 pages. Waiting for 2 seconds...\n",
      "Scraped 1620 pages. Waiting for 2 seconds...\n",
      "Scraped 1630 pages. Waiting for 2 seconds...\n",
      "Scraped 1640 pages. Waiting for 2 seconds...\n",
      "Scraped 1650 pages. Waiting for 2 seconds...\n",
      "Scraped 1660 pages. Waiting for 2 seconds...\n",
      "Scraped 1670 pages. Waiting for 2 seconds...\n",
      "Scraped 1680 pages. Waiting for 2 seconds...\n",
      "Scraped 1690 pages. Waiting for 2 seconds...\n",
      "Scraped 1700 pages. Waiting for 2 seconds...\n",
      "Scraped 1700 pages. Waiting for 5 minutes...\n",
      "Scraped 1710 pages. Waiting for 2 seconds...\n",
      "Scraped 1720 pages. Waiting for 2 seconds...\n",
      "Scraped 1730 pages. Waiting for 2 seconds...\n",
      "Scraped 1740 pages. Waiting for 2 seconds...\n",
      "Scraped 1750 pages. Waiting for 2 seconds...\n",
      "Scraped 1760 pages. Waiting for 2 seconds...\n",
      "Scraped 1770 pages. Waiting for 2 seconds...\n",
      "Scraped 1780 pages. Waiting for 2 seconds...\n",
      "Scraped 1790 pages. Waiting for 2 seconds...\n",
      "Scraped 1800 pages. Waiting for 2 seconds...\n",
      "Scraped 1800 pages. Waiting for 5 minutes...\n",
      "Scraped 1810 pages. Waiting for 2 seconds...\n",
      "Scraped 1820 pages. Waiting for 2 seconds...\n",
      "Scraped 1830 pages. Waiting for 2 seconds...\n",
      "Scraped 1840 pages. Waiting for 2 seconds...\n",
      "Scraped 1850 pages. Waiting for 2 seconds...\n",
      "Scraped 1860 pages. Waiting for 2 seconds...\n",
      "Scraped 1870 pages. Waiting for 2 seconds...\n",
      "Scraped 1880 pages. Waiting for 2 seconds...\n",
      "Scraped 1890 pages. Waiting for 2 seconds...\n",
      "Scraped 1900 pages. Waiting for 2 seconds...\n",
      "Scraped 1900 pages. Waiting for 5 minutes...\n",
      "Scraped 1910 pages. Waiting for 2 seconds...\n",
      "Scraped 1920 pages. Waiting for 2 seconds...\n",
      "Scraped 1930 pages. Waiting for 2 seconds...\n",
      "Scraped 1940 pages. Waiting for 2 seconds...\n",
      "Scraped 1950 pages. Waiting for 2 seconds...\n",
      "Scraped 1960 pages. Waiting for 2 seconds...\n",
      "Scraped 1970 pages. Waiting for 2 seconds...\n",
      "Scraped 1980 pages. Waiting for 2 seconds...\n",
      "Scraped 1990 pages. Waiting for 2 seconds...\n",
      "Scraped 2000 pages. Waiting for 2 seconds...\n",
      "Scraped 2000 pages. Waiting for 5 minutes...\n"
     ]
    }
   ],
   "source": [
    "start_page = 1001\n",
    "end_page = 2000\n",
    "frame2_reviews = scrape_pages(start_page, end_page)\n",
    "df2 = pd.DataFrame(frame2_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3736342-aaa0-49be-b1f1-264f5ad09af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "MAWN16X1uDXr",
   "metadata": {
    "id": "MAWN16X1uDXr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 2010 pages. Waiting for 2 seconds...\n",
      "Scraped 2020 pages. Waiting for 2 seconds...\n",
      "Scraped 2030 pages. Waiting for 2 seconds...\n",
      "Scraped 2040 pages. Waiting for 2 seconds...\n",
      "Scraped 2050 pages. Waiting for 2 seconds...\n",
      "Scraped 2060 pages. Waiting for 2 seconds...\n",
      "Scraped 2070 pages. Waiting for 2 seconds...\n",
      "Scraped 2080 pages. Waiting for 2 seconds...\n",
      "Scraped 2090 pages. Waiting for 2 seconds...\n",
      "Scraped 2100 pages. Waiting for 2 seconds...\n",
      "Scraped 2100 pages. Waiting for 5 minutes...\n",
      "Scraped 2110 pages. Waiting for 2 seconds...\n",
      "Scraped 2120 pages. Waiting for 2 seconds...\n",
      "Scraped 2130 pages. Waiting for 2 seconds...\n",
      "Scraped 2140 pages. Waiting for 2 seconds...\n",
      "Scraped 2150 pages. Waiting for 2 seconds...\n",
      "Scraped 2160 pages. Waiting for 2 seconds...\n",
      "Scraped 2170 pages. Waiting for 2 seconds...\n",
      "Scraped 2180 pages. Waiting for 2 seconds...\n",
      "Scraped 2190 pages. Waiting for 2 seconds...\n",
      "Scraped 2200 pages. Waiting for 2 seconds...\n",
      "Scraped 2200 pages. Waiting for 5 minutes...\n",
      "Scraped 2210 pages. Waiting for 2 seconds...\n",
      "Scraped 2220 pages. Waiting for 2 seconds...\n",
      "Scraped 2230 pages. Waiting for 2 seconds...\n",
      "Scraped 2240 pages. Waiting for 2 seconds...\n",
      "Scraped 2250 pages. Waiting for 2 seconds...\n",
      "Scraped 2260 pages. Waiting for 2 seconds...\n",
      "Scraped 2270 pages. Waiting for 2 seconds...\n",
      "Scraped 2280 pages. Waiting for 2 seconds...\n",
      "Scraped 2290 pages. Waiting for 2 seconds...\n",
      "Scraped 2300 pages. Waiting for 2 seconds...\n",
      "Scraped 2300 pages. Waiting for 5 minutes...\n",
      "Scraped 2310 pages. Waiting for 2 seconds...\n",
      "Scraped 2320 pages. Waiting for 2 seconds...\n",
      "Scraped 2330 pages. Waiting for 2 seconds...\n",
      "Scraped 2340 pages. Waiting for 2 seconds...\n",
      "Scraped 2350 pages. Waiting for 2 seconds...\n",
      "Scraped 2360 pages. Waiting for 2 seconds...\n",
      "Scraped 2370 pages. Waiting for 2 seconds...\n",
      "Scraped 2380 pages. Waiting for 2 seconds...\n",
      "Scraped 2390 pages. Waiting for 2 seconds...\n",
      "Scraped 2400 pages. Waiting for 2 seconds...\n",
      "Scraped 2400 pages. Waiting for 5 minutes...\n",
      "Scraped 2410 pages. Waiting for 2 seconds...\n",
      "Scraped 2420 pages. Waiting for 2 seconds...\n",
      "Scraped 2430 pages. Waiting for 2 seconds...\n",
      "Scraped 2440 pages. Waiting for 2 seconds...\n",
      "Scraped 2450 pages. Waiting for 2 seconds...\n",
      "Scraped 2460 pages. Waiting for 2 seconds...\n",
      "Scraped 2470 pages. Waiting for 2 seconds...\n",
      "Scraped 2480 pages. Waiting for 2 seconds...\n",
      "Scraped 2490 pages. Waiting for 2 seconds...\n",
      "Scraped 2500 pages. Waiting for 2 seconds...\n",
      "Scraped 2500 pages. Waiting for 5 minutes...\n",
      "Scraped 2510 pages. Waiting for 2 seconds...\n",
      "Scraped 2520 pages. Waiting for 2 seconds...\n",
      "Scraped 2530 pages. Waiting for 2 seconds...\n",
      "Scraped 2540 pages. Waiting for 2 seconds...\n",
      "Scraped 2550 pages. Waiting for 2 seconds...\n",
      "Scraped 2560 pages. Waiting for 2 seconds...\n",
      "Scraped 2570 pages. Waiting for 2 seconds...\n",
      "Scraped 2580 pages. Waiting for 2 seconds...\n",
      "Scraped 2590 pages. Waiting for 2 seconds...\n",
      "Scraped 2600 pages. Waiting for 2 seconds...\n",
      "Scraped 2600 pages. Waiting for 5 minutes...\n",
      "Scraped 2610 pages. Waiting for 2 seconds...\n",
      "Scraped 2620 pages. Waiting for 2 seconds...\n",
      "Scraped 2630 pages. Waiting for 2 seconds...\n",
      "Scraped 2640 pages. Waiting for 2 seconds...\n",
      "Scraped 2650 pages. Waiting for 2 seconds...\n",
      "Scraped 2660 pages. Waiting for 2 seconds...\n",
      "Scraped 2670 pages. Waiting for 2 seconds...\n",
      "Scraped 2680 pages. Waiting for 2 seconds...\n",
      "Scraped 2690 pages. Waiting for 2 seconds...\n",
      "Scraped 2700 pages. Waiting for 2 seconds...\n",
      "Scraped 2700 pages. Waiting for 5 minutes...\n",
      "Scraped 2710 pages. Waiting for 2 seconds...\n",
      "Scraped 2720 pages. Waiting for 2 seconds...\n",
      "Scraped 2730 pages. Waiting for 2 seconds...\n",
      "Scraped 2740 pages. Waiting for 2 seconds...\n",
      "Scraped 2750 pages. Waiting for 2 seconds...\n",
      "Scraped 2760 pages. Waiting for 2 seconds...\n",
      "Scraped 2770 pages. Waiting for 2 seconds...\n",
      "Scraped 2780 pages. Waiting for 2 seconds...\n",
      "Scraped 2790 pages. Waiting for 2 seconds...\n",
      "Scraped 2800 pages. Waiting for 2 seconds...\n",
      "Scraped 2800 pages. Waiting for 5 minutes...\n",
      "Scraped 2810 pages. Waiting for 2 seconds...\n",
      "Scraped 2820 pages. Waiting for 2 seconds...\n",
      "Scraped 2830 pages. Waiting for 2 seconds...\n",
      "Scraped 2840 pages. Waiting for 2 seconds...\n",
      "Scraped 2850 pages. Waiting for 2 seconds...\n",
      "Scraped 2860 pages. Waiting for 2 seconds...\n",
      "Scraped 2870 pages. Waiting for 2 seconds...\n",
      "Scraped 2880 pages. Waiting for 2 seconds...\n",
      "Scraped 2890 pages. Waiting for 2 seconds...\n",
      "Scraped 2900 pages. Waiting for 2 seconds...\n",
      "Scraped 2900 pages. Waiting for 5 minutes...\n",
      "Scraped 2910 pages. Waiting for 2 seconds...\n",
      "Scraped 2920 pages. Waiting for 2 seconds...\n",
      "Scraped 2930 pages. Waiting for 2 seconds...\n",
      "Scraped 2940 pages. Waiting for 2 seconds...\n",
      "Scraped 2950 pages. Waiting for 2 seconds...\n",
      "Scraped 2960 pages. Waiting for 2 seconds...\n",
      "Scraped 2970 pages. Waiting for 2 seconds...\n",
      "Scraped 2980 pages. Waiting for 2 seconds...\n",
      "Scraped 2990 pages. Waiting for 2 seconds...\n",
      "Scraped 3000 pages. Waiting for 2 seconds...\n",
      "Scraped 3000 pages. Waiting for 5 minutes...\n"
     ]
    }
   ],
   "source": [
    "start_page = 2001\n",
    "end_page = 3000\n",
    "frame3_reviews = scrape_pages(start_page, end_page)\n",
    "df3 = pd.DataFrame(frame3_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb957c8a-c291-4853-ab2c-ada1e524c158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 9)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e11ad51-5623-44c1-a897-8a6c2ddebe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf1= pd.concat([df1, df2, df3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19cab666-4460-4b37-a619-258d36d1b714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 9)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f0db189-1b24-483d-9755-8c366c3d15dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf1.to_csv('reviews_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "Fx8yjWlWuO5m",
   "metadata": {
    "id": "Fx8yjWlWuO5m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 3010 pages. Waiting for 2 seconds...\n",
      "Scraped 3020 pages. Waiting for 2 seconds...\n",
      "Scraped 3030 pages. Waiting for 2 seconds...\n",
      "Scraped 3040 pages. Waiting for 2 seconds...\n",
      "Scraped 3050 pages. Waiting for 2 seconds...\n",
      "Scraped 3060 pages. Waiting for 2 seconds...\n",
      "Scraped 3070 pages. Waiting for 2 seconds...\n",
      "Scraped 3080 pages. Waiting for 2 seconds...\n",
      "Scraped 3090 pages. Waiting for 2 seconds...\n",
      "Scraped 3100 pages. Waiting for 2 seconds...\n",
      "Scraped 3100 pages. Waiting for 5 minutes...\n",
      "Scraped 3110 pages. Waiting for 2 seconds...\n",
      "Scraped 3120 pages. Waiting for 2 seconds...\n",
      "Scraped 3130 pages. Waiting for 2 seconds...\n",
      "Scraped 3140 pages. Waiting for 2 seconds...\n",
      "Scraped 3150 pages. Waiting for 2 seconds...\n",
      "Scraped 3160 pages. Waiting for 2 seconds...\n",
      "Scraped 3170 pages. Waiting for 2 seconds...\n",
      "Scraped 3180 pages. Waiting for 2 seconds...\n",
      "Scraped 3190 pages. Waiting for 2 seconds...\n",
      "Scraped 3200 pages. Waiting for 2 seconds...\n",
      "Scraped 3200 pages. Waiting for 5 minutes...\n",
      "Scraped 3210 pages. Waiting for 2 seconds...\n",
      "Scraped 3220 pages. Waiting for 2 seconds...\n",
      "Scraped 3230 pages. Waiting for 2 seconds...\n",
      "Scraped 3240 pages. Waiting for 2 seconds...\n",
      "Scraped 3250 pages. Waiting for 2 seconds...\n",
      "Scraped 3260 pages. Waiting for 2 seconds...\n",
      "Scraped 3270 pages. Waiting for 2 seconds...\n",
      "Scraped 3280 pages. Waiting for 2 seconds...\n",
      "Scraped 3290 pages. Waiting for 2 seconds...\n",
      "Scraped 3300 pages. Waiting for 2 seconds...\n",
      "Scraped 3300 pages. Waiting for 5 minutes...\n",
      "Scraped 3310 pages. Waiting for 2 seconds...\n",
      "Scraped 3320 pages. Waiting for 2 seconds...\n",
      "Scraped 3330 pages. Waiting for 2 seconds...\n",
      "Scraped 3340 pages. Waiting for 2 seconds...\n",
      "Scraped 3350 pages. Waiting for 2 seconds...\n",
      "Scraped 3360 pages. Waiting for 2 seconds...\n",
      "Scraped 3370 pages. Waiting for 2 seconds...\n",
      "Scraped 3380 pages. Waiting for 2 seconds...\n",
      "Scraped 3390 pages. Waiting for 2 seconds...\n",
      "Scraped 3400 pages. Waiting for 2 seconds...\n",
      "Scraped 3400 pages. Waiting for 5 minutes...\n",
      "Scraped 3410 pages. Waiting for 2 seconds...\n",
      "Scraped 3420 pages. Waiting for 2 seconds...\n",
      "Scraped 3430 pages. Waiting for 2 seconds...\n",
      "Scraped 3440 pages. Waiting for 2 seconds...\n",
      "Scraped 3450 pages. Waiting for 2 seconds...\n",
      "Scraped 3460 pages. Waiting for 2 seconds...\n",
      "Scraped 3470 pages. Waiting for 2 seconds...\n",
      "Scraped 3480 pages. Waiting for 2 seconds...\n",
      "Scraped 3490 pages. Waiting for 2 seconds...\n",
      "Scraped 3500 pages. Waiting for 2 seconds...\n",
      "Scraped 3500 pages. Waiting for 5 minutes...\n",
      "Scraped 3510 pages. Waiting for 2 seconds...\n",
      "Scraped 3520 pages. Waiting for 2 seconds...\n",
      "Scraped 3530 pages. Waiting for 2 seconds...\n",
      "Scraped 3540 pages. Waiting for 2 seconds...\n",
      "Scraped 3550 pages. Waiting for 2 seconds...\n",
      "Scraped 3560 pages. Waiting for 2 seconds...\n",
      "Scraped 3570 pages. Waiting for 2 seconds...\n",
      "Scraped 3580 pages. Waiting for 2 seconds...\n",
      "Scraped 3590 pages. Waiting for 2 seconds...\n",
      "Scraped 3600 pages. Waiting for 2 seconds...\n",
      "Scraped 3600 pages. Waiting for 5 minutes...\n",
      "Scraped 3610 pages. Waiting for 2 seconds...\n",
      "Scraped 3620 pages. Waiting for 2 seconds...\n",
      "Scraped 3630 pages. Waiting for 2 seconds...\n",
      "Scraped 3640 pages. Waiting for 2 seconds...\n",
      "Scraped 3650 pages. Waiting for 2 seconds...\n",
      "Scraped 3660 pages. Waiting for 2 seconds...\n",
      "Scraped 3670 pages. Waiting for 2 seconds...\n",
      "Scraped 3680 pages. Waiting for 2 seconds...\n",
      "Scraped 3690 pages. Waiting for 2 seconds...\n",
      "Scraped 3700 pages. Waiting for 2 seconds...\n",
      "Scraped 3700 pages. Waiting for 5 minutes...\n",
      "Scraped 3710 pages. Waiting for 2 seconds...\n",
      "Scraped 3720 pages. Waiting for 2 seconds...\n",
      "Scraped 3730 pages. Waiting for 2 seconds...\n",
      "Scraped 3740 pages. Waiting for 2 seconds...\n",
      "Scraped 3750 pages. Waiting for 2 seconds...\n",
      "Scraped 3760 pages. Waiting for 2 seconds...\n",
      "Scraped 3770 pages. Waiting for 2 seconds...\n",
      "Scraped 3780 pages. Waiting for 2 seconds...\n",
      "Scraped 3790 pages. Waiting for 2 seconds...\n",
      "Scraped 3800 pages. Waiting for 2 seconds...\n",
      "Scraped 3800 pages. Waiting for 5 minutes...\n",
      "Scraped 3810 pages. Waiting for 2 seconds...\n",
      "Scraped 3820 pages. Waiting for 2 seconds...\n",
      "Scraped 3830 pages. Waiting for 2 seconds...\n",
      "Scraped 3840 pages. Waiting for 2 seconds...\n",
      "Scraped 3850 pages. Waiting for 2 seconds...\n",
      "Scraped 3860 pages. Waiting for 2 seconds...\n",
      "Scraped 3870 pages. Waiting for 2 seconds...\n",
      "Scraped 3880 pages. Waiting for 2 seconds...\n",
      "Scraped 3890 pages. Waiting for 2 seconds...\n",
      "Scraped 3900 pages. Waiting for 2 seconds...\n",
      "Scraped 3900 pages. Waiting for 5 minutes...\n",
      "Scraped 3910 pages. Waiting for 2 seconds...\n",
      "Scraped 3920 pages. Waiting for 2 seconds...\n",
      "Scraped 3930 pages. Waiting for 2 seconds...\n",
      "Scraped 3940 pages. Waiting for 2 seconds...\n",
      "Scraped 3950 pages. Waiting for 2 seconds...\n",
      "Scraped 3960 pages. Waiting for 2 seconds...\n",
      "Scraped 3970 pages. Waiting for 2 seconds...\n",
      "Scraped 3980 pages. Waiting for 2 seconds...\n",
      "Scraped 3990 pages. Waiting for 2 seconds...\n",
      "Scraped 4000 pages. Waiting for 2 seconds...\n",
      "Scraped 4000 pages. Waiting for 5 minutes...\n"
     ]
    }
   ],
   "source": [
    "start_page = 3001\n",
    "end_page = 4000\n",
    "frame4_reviews = scrape_pages(start_page, end_page)\n",
    "df4 = pd.DataFrame(frame4_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4db152e8-cf8a-476c-9c45-237da08dd7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19980, 9)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dfaac679-b135-40e7-bf03-edbac7b41113",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf2= pd.concat([df1, df2, df3,df4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db90f0a0-c871-4ee5-af99-994c02eac6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79980, 9)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1df8986d-3aa4-4b85-b7e3-cf4495089ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf2.to_csv('reviews_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e66051a-f6d5-41d0-a0eb-cc3adcff9a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79980, 9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "tt7wzJEouQ0h",
   "metadata": {
    "id": "tt7wzJEouQ0h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 4010 pages. Waiting for 2 seconds...\n",
      "Scraped 4020 pages. Waiting for 2 seconds...\n",
      "Scraped 4030 pages. Waiting for 2 seconds...\n",
      "Scraped 4040 pages. Waiting for 2 seconds...\n",
      "Scraped 4050 pages. Waiting for 2 seconds...\n",
      "Scraped 4060 pages. Waiting for 2 seconds...\n",
      "Scraped 4070 pages. Waiting for 2 seconds...\n",
      "Scraped 4080 pages. Waiting for 2 seconds...\n",
      "Scraped 4090 pages. Waiting for 2 seconds...\n",
      "Scraped 4100 pages. Waiting for 2 seconds...\n",
      "Scraped 4100 pages. Waiting for 5 minutes...\n",
      "Scraped 4110 pages. Waiting for 2 seconds...\n",
      "Scraped 4120 pages. Waiting for 2 seconds...\n",
      "Scraped 4130 pages. Waiting for 2 seconds...\n",
      "Scraped 4140 pages. Waiting for 2 seconds...\n",
      "Scraped 4150 pages. Waiting for 2 seconds...\n",
      "Scraped 4160 pages. Waiting for 2 seconds...\n",
      "Scraped 4170 pages. Waiting for 2 seconds...\n",
      "Scraped 4180 pages. Waiting for 2 seconds...\n",
      "Scraped 4190 pages. Waiting for 2 seconds...\n",
      "Scraped 4200 pages. Waiting for 2 seconds...\n",
      "Scraped 4200 pages. Waiting for 5 minutes...\n",
      "Scraped 4210 pages. Waiting for 2 seconds...\n",
      "Scraped 4220 pages. Waiting for 2 seconds...\n",
      "Scraped 4230 pages. Waiting for 2 seconds...\n",
      "Scraped 4240 pages. Waiting for 2 seconds...\n",
      "Scraped 4250 pages. Waiting for 2 seconds...\n",
      "Scraped 4260 pages. Waiting for 2 seconds...\n",
      "Scraped 4270 pages. Waiting for 2 seconds...\n",
      "Scraped 4280 pages. Waiting for 2 seconds...\n",
      "Scraped 4290 pages. Waiting for 2 seconds...\n",
      "Scraped 4300 pages. Waiting for 2 seconds...\n",
      "Scraped 4300 pages. Waiting for 5 minutes...\n",
      "Scraped 4310 pages. Waiting for 2 seconds...\n",
      "Scraped 4320 pages. Waiting for 2 seconds...\n",
      "Scraped 4330 pages. Waiting for 2 seconds...\n",
      "Scraped 4340 pages. Waiting for 2 seconds...\n",
      "Scraped 4350 pages. Waiting for 2 seconds...\n",
      "Scraped 4360 pages. Waiting for 2 seconds...\n",
      "Scraped 4370 pages. Waiting for 2 seconds...\n",
      "Scraped 4380 pages. Waiting for 2 seconds...\n",
      "Scraped 4390 pages. Waiting for 2 seconds...\n",
      "Scraped 4400 pages. Waiting for 2 seconds...\n",
      "Scraped 4400 pages. Waiting for 5 minutes...\n",
      "Scraped 4410 pages. Waiting for 2 seconds...\n",
      "Scraped 4420 pages. Waiting for 2 seconds...\n",
      "Scraped 4430 pages. Waiting for 2 seconds...\n",
      "Scraped 4440 pages. Waiting for 2 seconds...\n",
      "Scraped 4450 pages. Waiting for 2 seconds...\n",
      "Scraped 4460 pages. Waiting for 2 seconds...\n",
      "Scraped 4470 pages. Waiting for 2 seconds...\n",
      "Scraped 4480 pages. Waiting for 2 seconds...\n",
      "Scraped 4490 pages. Waiting for 2 seconds...\n",
      "Scraped 4500 pages. Waiting for 2 seconds...\n",
      "Scraped 4500 pages. Waiting for 5 minutes...\n",
      "Scraped 4510 pages. Waiting for 2 seconds...\n",
      "Scraped 4520 pages. Waiting for 2 seconds...\n",
      "Scraped 4530 pages. Waiting for 2 seconds...\n",
      "Scraped 4540 pages. Waiting for 2 seconds...\n",
      "Scraped 4550 pages. Waiting for 2 seconds...\n",
      "Scraped 4560 pages. Waiting for 2 seconds...\n",
      "Scraped 4570 pages. Waiting for 2 seconds...\n",
      "Scraped 4580 pages. Waiting for 2 seconds...\n",
      "Scraped 4590 pages. Waiting for 2 seconds...\n",
      "Scraped 4600 pages. Waiting for 2 seconds...\n",
      "Scraped 4600 pages. Waiting for 5 minutes...\n",
      "Scraped 4610 pages. Waiting for 2 seconds...\n",
      "Scraped 4620 pages. Waiting for 2 seconds...\n",
      "Scraped 4630 pages. Waiting for 2 seconds...\n",
      "Scraped 4640 pages. Waiting for 2 seconds...\n",
      "Scraped 4650 pages. Waiting for 2 seconds...\n",
      "Scraped 4660 pages. Waiting for 2 seconds...\n",
      "Scraped 4670 pages. Waiting for 2 seconds...\n",
      "Scraped 4680 pages. Waiting for 2 seconds...\n",
      "Scraped 4690 pages. Waiting for 2 seconds...\n",
      "Scraped 4700 pages. Waiting for 2 seconds...\n",
      "Scraped 4700 pages. Waiting for 5 minutes...\n",
      "Scraped 4710 pages. Waiting for 2 seconds...\n",
      "Scraped 4720 pages. Waiting for 2 seconds...\n",
      "Scraped 4730 pages. Waiting for 2 seconds...\n",
      "Scraped 4740 pages. Waiting for 2 seconds...\n",
      "Scraped 4750 pages. Waiting for 2 seconds...\n",
      "Scraped 4760 pages. Waiting for 2 seconds...\n",
      "Scraped 4770 pages. Waiting for 2 seconds...\n",
      "Scraped 4780 pages. Waiting for 2 seconds...\n",
      "Scraped 4790 pages. Waiting for 2 seconds...\n",
      "Scraped 4800 pages. Waiting for 2 seconds...\n",
      "Scraped 4800 pages. Waiting for 5 minutes...\n",
      "Scraped 4810 pages. Waiting for 2 seconds...\n",
      "Scraped 4820 pages. Waiting for 2 seconds...\n",
      "Scraped 4830 pages. Waiting for 2 seconds...\n",
      "Scraped 4840 pages. Waiting for 2 seconds...\n",
      "Scraped 4850 pages. Waiting for 2 seconds...\n",
      "Scraped 4860 pages. Waiting for 2 seconds...\n",
      "Scraped 4870 pages. Waiting for 2 seconds...\n",
      "Scraped 4880 pages. Waiting for 2 seconds...\n",
      "Scraped 4890 pages. Waiting for 2 seconds...\n",
      "Scraped 4900 pages. Waiting for 2 seconds...\n",
      "Scraped 4900 pages. Waiting for 5 minutes...\n",
      "Scraped 4910 pages. Waiting for 2 seconds...\n",
      "Scraped 4920 pages. Waiting for 2 seconds...\n",
      "Scraped 4930 pages. Waiting for 2 seconds...\n",
      "Scraped 4940 pages. Waiting for 2 seconds...\n",
      "Scraped 4950 pages. Waiting for 2 seconds...\n",
      "Scraped 4960 pages. Waiting for 2 seconds...\n",
      "Scraped 4970 pages. Waiting for 2 seconds...\n",
      "Scraped 4980 pages. Waiting for 2 seconds...\n",
      "Scraped 4990 pages. Waiting for 2 seconds...\n",
      "Scraped 5000 pages. Waiting for 2 seconds...\n",
      "Scraped 5000 pages. Waiting for 5 minutes...\n"
     ]
    }
   ],
   "source": [
    "start_page = 4001\n",
    "end_page = 5000\n",
    "frame5_reviews = scrape_pages(start_page, end_page)\n",
    "df5 = pd.DataFrame(frame5_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a73c065e-4b9c-439a-bb00-6549a9f91ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 9)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "-bQt-jzEuSyF",
   "metadata": {
    "id": "-bQt-jzEuSyF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 5010 pages. Waiting for 2 seconds...\n",
      "Scraped 5020 pages. Waiting for 2 seconds...\n",
      "Scraped 5030 pages. Waiting for 2 seconds...\n",
      "Scraped 5040 pages. Waiting for 2 seconds...\n",
      "Scraped 5050 pages. Waiting for 2 seconds...\n",
      "Scraped 5060 pages. Waiting for 2 seconds...\n",
      "Scraped 5070 pages. Waiting for 2 seconds...\n",
      "Scraped 5080 pages. Waiting for 2 seconds...\n",
      "Scraped 5090 pages. Waiting for 2 seconds...\n",
      "Scraped 5100 pages. Waiting for 2 seconds...\n",
      "Scraped 5100 pages. Waiting for 5 minutes...\n",
      "Scraped 5110 pages. Waiting for 2 seconds...\n",
      "Scraped 5120 pages. Waiting for 2 seconds...\n",
      "Scraped 5130 pages. Waiting for 2 seconds...\n",
      "Scraped 5140 pages. Waiting for 2 seconds...\n",
      "Scraped 5150 pages. Waiting for 2 seconds...\n",
      "Scraped 5160 pages. Waiting for 2 seconds...\n",
      "Scraped 5170 pages. Waiting for 2 seconds...\n",
      "Scraped 5180 pages. Waiting for 2 seconds...\n",
      "Scraped 5190 pages. Waiting for 2 seconds...\n",
      "Scraped 5200 pages. Waiting for 2 seconds...\n",
      "Scraped 5200 pages. Waiting for 5 minutes...\n",
      "Scraped 5210 pages. Waiting for 2 seconds...\n",
      "Scraped 5220 pages. Waiting for 2 seconds...\n",
      "Scraped 5230 pages. Waiting for 2 seconds...\n",
      "Scraped 5240 pages. Waiting for 2 seconds...\n",
      "Scraped 5250 pages. Waiting for 2 seconds...\n",
      "Scraped 5260 pages. Waiting for 2 seconds...\n",
      "Scraped 5270 pages. Waiting for 2 seconds...\n",
      "Scraped 5280 pages. Waiting for 2 seconds...\n",
      "Scraped 5290 pages. Waiting for 2 seconds...\n",
      "Scraped 5300 pages. Waiting for 2 seconds...\n",
      "Scraped 5300 pages. Waiting for 5 minutes...\n",
      "Scraped 5310 pages. Waiting for 2 seconds...\n",
      "Scraped 5320 pages. Waiting for 2 seconds...\n",
      "Scraped 5330 pages. Waiting for 2 seconds...\n",
      "Scraped 5340 pages. Waiting for 2 seconds...\n",
      "Scraped 5350 pages. Waiting for 2 seconds...\n",
      "Scraped 5360 pages. Waiting for 2 seconds...\n",
      "Scraped 5370 pages. Waiting for 2 seconds...\n",
      "Scraped 5380 pages. Waiting for 2 seconds...\n",
      "Scraped 5390 pages. Waiting for 2 seconds...\n",
      "Scraped 5400 pages. Waiting for 2 seconds...\n",
      "Scraped 5400 pages. Waiting for 5 minutes...\n",
      "Scraped 5410 pages. Waiting for 2 seconds...\n",
      "Scraped 5420 pages. Waiting for 2 seconds...\n",
      "Scraped 5430 pages. Waiting for 2 seconds...\n",
      "Scraped 5440 pages. Waiting for 2 seconds...\n",
      "Scraped 5450 pages. Waiting for 2 seconds...\n",
      "Scraped 5460 pages. Waiting for 2 seconds...\n",
      "Scraped 5470 pages. Waiting for 2 seconds...\n",
      "Scraped 5480 pages. Waiting for 2 seconds...\n",
      "Scraped 5490 pages. Waiting for 2 seconds...\n",
      "Scraped 5500 pages. Waiting for 2 seconds...\n",
      "Scraped 5500 pages. Waiting for 5 minutes...\n",
      "Scraped 5510 pages. Waiting for 2 seconds...\n",
      "Scraped 5520 pages. Waiting for 2 seconds...\n",
      "Scraped 5530 pages. Waiting for 2 seconds...\n",
      "Scraped 5540 pages. Waiting for 2 seconds...\n",
      "Scraped 5550 pages. Waiting for 2 seconds...\n",
      "Scraped 5560 pages. Waiting for 2 seconds...\n",
      "Scraped 5570 pages. Waiting for 2 seconds...\n",
      "Scraped 5580 pages. Waiting for 2 seconds...\n",
      "Scraped 5590 pages. Waiting for 2 seconds...\n",
      "Scraped 5600 pages. Waiting for 2 seconds...\n",
      "Scraped 5600 pages. Waiting for 5 minutes...\n",
      "Scraped 5610 pages. Waiting for 2 seconds...\n",
      "Scraped 5620 pages. Waiting for 2 seconds...\n",
      "Scraped 5630 pages. Waiting for 2 seconds...\n",
      "Scraped 5640 pages. Waiting for 2 seconds...\n",
      "Scraped 5650 pages. Waiting for 2 seconds...\n",
      "Scraped 5660 pages. Waiting for 2 seconds...\n",
      "Scraped 5670 pages. Waiting for 2 seconds...\n",
      "Scraped 5680 pages. Waiting for 2 seconds...\n",
      "Scraped 5690 pages. Waiting for 2 seconds...\n",
      "Scraped 5700 pages. Waiting for 2 seconds...\n",
      "Scraped 5700 pages. Waiting for 5 minutes...\n",
      "Scraped 5710 pages. Waiting for 2 seconds...\n",
      "Scraped 5720 pages. Waiting for 2 seconds...\n",
      "Scraped 5730 pages. Waiting for 2 seconds...\n",
      "Scraped 5740 pages. Waiting for 2 seconds...\n",
      "Scraped 5750 pages. Waiting for 2 seconds...\n",
      "Scraped 5760 pages. Waiting for 2 seconds...\n",
      "Scraped 5770 pages. Waiting for 2 seconds...\n",
      "Scraped 5780 pages. Waiting for 2 seconds...\n",
      "Scraped 5790 pages. Waiting for 2 seconds...\n",
      "Scraped 5800 pages. Waiting for 2 seconds...\n",
      "Scraped 5800 pages. Waiting for 5 minutes...\n",
      "Scraped 5810 pages. Waiting for 2 seconds...\n",
      "Scraped 5820 pages. Waiting for 2 seconds...\n",
      "Scraped 5830 pages. Waiting for 2 seconds...\n",
      "Scraped 5840 pages. Waiting for 2 seconds...\n",
      "Scraped 5850 pages. Waiting for 2 seconds...\n",
      "Scraped 5860 pages. Waiting for 2 seconds...\n",
      "Scraped 5870 pages. Waiting for 2 seconds...\n",
      "Scraped 5880 pages. Waiting for 2 seconds...\n",
      "Scraped 5890 pages. Waiting for 2 seconds...\n",
      "Scraped 5900 pages. Waiting for 2 seconds...\n",
      "Scraped 5900 pages. Waiting for 5 minutes...\n",
      "Scraped 5910 pages. Waiting for 2 seconds...\n",
      "Scraped 5920 pages. Waiting for 2 seconds...\n",
      "Scraped 5930 pages. Waiting for 2 seconds...\n",
      "Scraped 5940 pages. Waiting for 2 seconds...\n",
      "Scraped 5950 pages. Waiting for 2 seconds...\n",
      "Scraped 5960 pages. Waiting for 2 seconds...\n",
      "Scraped 5970 pages. Waiting for 2 seconds...\n",
      "Scraped 5980 pages. Waiting for 2 seconds...\n",
      "Scraped 5990 pages. Waiting for 2 seconds...\n",
      "Scraped 6000 pages. Waiting for 2 seconds...\n",
      "Scraped 6000 pages. Waiting for 5 minutes...\n"
     ]
    }
   ],
   "source": [
    "start_page = 5001\n",
    "end_page = 6000\n",
    "frame6_reviews = scrape_pages(start_page, end_page)\n",
    "df6 = pd.DataFrame(frame6_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b77469c-24c4-4dc4-aca0-a88147a0e8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 6010 pages. Waiting for 2 seconds...\n",
      "Scraped 6020 pages. Waiting for 2 seconds...\n",
      "Scraped 6030 pages. Waiting for 2 seconds...\n",
      "Scraped 6040 pages. Waiting for 2 seconds...\n",
      "Scraped 6050 pages. Waiting for 2 seconds...\n",
      "Scraped 6060 pages. Waiting for 2 seconds...\n",
      "Scraped 6070 pages. Waiting for 2 seconds...\n",
      "Scraped 6080 pages. Waiting for 2 seconds...\n",
      "Scraped 6090 pages. Waiting for 2 seconds...\n",
      "Scraped 6100 pages. Waiting for 2 seconds...\n",
      "Scraped 6100 pages. Waiting for 5 minutes...\n"
     ]
    }
   ],
   "source": [
    "start_page = 6001\n",
    "end_page = 6100\n",
    "frame7_reviews = scrape_pages(start_page, end_page)\n",
    "df7 = pd.DataFrame(frame7_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0159825a-1316-4e4b-bdf3-b1f412eafadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 6110 pages. Waiting for 2 seconds...\n",
      "Scraped 6120 pages. Waiting for 2 seconds...\n",
      "Scraped 6130 pages. Waiting for 2 seconds...\n",
      "Scraped 6140 pages. Waiting for 2 seconds...\n",
      "Scraped 6150 pages. Waiting for 2 seconds...\n",
      "Scraped 6160 pages. Waiting for 2 seconds...\n",
      "Scraped 6170 pages. Waiting for 2 seconds...\n",
      "Scraped 6180 pages. Waiting for 2 seconds...\n",
      "Scraped 6190 pages. Waiting for 2 seconds...\n",
      "Scraped 6200 pages. Waiting for 2 seconds...\n",
      "Scraped 6200 pages. Waiting for 5 minutes...\n",
      "Scraped 6210 pages. Waiting for 2 seconds...\n",
      "Scraped 6220 pages. Waiting for 2 seconds...\n",
      "Scraped 6230 pages. Waiting for 2 seconds...\n",
      "Scraped 6240 pages. Waiting for 2 seconds...\n",
      "Scraped 6250 pages. Waiting for 2 seconds...\n",
      "Scraped 6260 pages. Waiting for 2 seconds...\n",
      "Scraped 6270 pages. Waiting for 2 seconds...\n",
      "Scraped 6280 pages. Waiting for 2 seconds...\n",
      "Scraped 6290 pages. Waiting for 2 seconds...\n",
      "Scraped 6300 pages. Waiting for 2 seconds...\n",
      "Scraped 6300 pages. Waiting for 5 minutes...\n",
      "Scraped 6310 pages. Waiting for 2 seconds...\n",
      "Scraped 6320 pages. Waiting for 2 seconds...\n",
      "Scraped 6330 pages. Waiting for 2 seconds...\n",
      "Scraped 6340 pages. Waiting for 2 seconds...\n",
      "Scraped 6350 pages. Waiting for 2 seconds...\n",
      "Scraped 6360 pages. Waiting for 2 seconds...\n",
      "Scraped 6370 pages. Waiting for 2 seconds...\n",
      "Scraped 6380 pages. Waiting for 2 seconds...\n",
      "Scraped 6390 pages. Waiting for 2 seconds...\n",
      "Scraped 6400 pages. Waiting for 2 seconds...\n",
      "Scraped 6400 pages. Waiting for 5 minutes...\n",
      "Scraped 6410 pages. Waiting for 2 seconds...\n",
      "Scraped 6420 pages. Waiting for 2 seconds...\n",
      "Scraped 6430 pages. Waiting for 2 seconds...\n",
      "Scraped 6440 pages. Waiting for 2 seconds...\n",
      "Scraped 6450 pages. Waiting for 2 seconds...\n",
      "Scraped 6460 pages. Waiting for 2 seconds...\n",
      "Scraped 6470 pages. Waiting for 2 seconds...\n",
      "Scraped 6480 pages. Waiting for 2 seconds...\n",
      "Scraped 6490 pages. Waiting for 2 seconds...\n",
      "Scraped 6500 pages. Waiting for 2 seconds...\n",
      "Scraped 6500 pages. Waiting for 5 minutes...\n",
      "Scraped 6510 pages. Waiting for 2 seconds...\n",
      "Scraped 6520 pages. Waiting for 2 seconds...\n",
      "Scraped 6530 pages. Waiting for 2 seconds...\n",
      "Scraped 6540 pages. Waiting for 2 seconds...\n",
      "Scraped 6550 pages. Waiting for 2 seconds...\n",
      "Scraped 6560 pages. Waiting for 2 seconds...\n",
      "Scraped 6570 pages. Waiting for 2 seconds...\n",
      "Scraped 6580 pages. Waiting for 2 seconds...\n",
      "Scraped 6590 pages. Waiting for 2 seconds...\n",
      "Scraped 6600 pages. Waiting for 2 seconds...\n",
      "Scraped 6600 pages. Waiting for 5 minutes...\n"
     ]
    }
   ],
   "source": [
    "start_page = 6101\n",
    "end_page = 6600\n",
    "frame8_reviews = scrape_pages(start_page, end_page)\n",
    "df8 = pd.DataFrame(frame8_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "OMAz6pscuXLm",
   "metadata": {
    "id": "OMAz6pscuXLm"
   },
   "outputs": [],
   "source": [
    "df=pd.concat([df1, df2, df3, df4, df5, df6,df7,df8], ignore_index=True)\n",
    "df.to_csv('fashionnova_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "qlXSmhNlujAq",
   "metadata": {
    "id": "qlXSmhNlujAq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131980, 9)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2bfae0e5-0d9d-45e6-aef6-7664e786251d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviewer Name</th>\n",
       "      <th>Profile Link</th>\n",
       "      <th>Country</th>\n",
       "      <th>Review Count</th>\n",
       "      <th>Review Date</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Date of Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23772</th>\n",
       "      <td>Nina Sanchez</td>\n",
       "      <td>/users/63e3c7fcf9dca00015f9f00a</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>2023-02-08T18:04:14.000Z</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>The website has great deals</td>\n",
       "      <td>Review text not found</td>\n",
       "      <td>February 08, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99847</th>\n",
       "      <td>Teyana Hudson</td>\n",
       "      <td>/users/614a151187c9cb001243f2cd</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>2021-09-21T17:23:31.000Z</td>\n",
       "      <td>Rated 3 out of 5 stars</td>\n",
       "      <td>You guys need a better cancellation…</td>\n",
       "      <td>You guys need a better cancellation service.</td>\n",
       "      <td>September 21, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22028</th>\n",
       "      <td>customer</td>\n",
       "      <td>/users/60bf7b67f78f7400197ca739</td>\n",
       "      <td>US</td>\n",
       "      <td>3 reviews</td>\n",
       "      <td>2023-02-28T20:06:58.000Z</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>Easy site to navigate</td>\n",
       "      <td>Review text not found</td>\n",
       "      <td>February 28, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20917</th>\n",
       "      <td>Savoy Jenkins</td>\n",
       "      <td>/users/62fbc6437fcced0012d098ab</td>\n",
       "      <td>US</td>\n",
       "      <td>2 reviews</td>\n",
       "      <td>2023-03-14T09:03:30.000Z</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>I like fashion nova</td>\n",
       "      <td>I like fashion nova. Easy to look thru to find...</td>\n",
       "      <td>March 14, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118420</th>\n",
       "      <td>Julian Bono</td>\n",
       "      <td>/users/60fed8eed22b22001257a37a</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>2021-07-26T15:46:54.000Z</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>Great</td>\n",
       "      <td>Great order!!!</td>\n",
       "      <td>July 26, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67629</th>\n",
       "      <td>Priscilla  Davis</td>\n",
       "      <td>/users/618ad332175ab400159163d6</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>2021-11-09T19:59:47.000Z</td>\n",
       "      <td>Rated 1 out of 5 stars</td>\n",
       "      <td>Hard time checking out</td>\n",
       "      <td>Review text not found</td>\n",
       "      <td>November 09, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118341</th>\n",
       "      <td>Jizzel tingling</td>\n",
       "      <td>/users/60ff0287f54aad0012de388e</td>\n",
       "      <td>JM</td>\n",
       "      <td>1 review</td>\n",
       "      <td>2021-07-26T18:44:23.000Z</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>I love fn</td>\n",
       "      <td>I love me some nova</td>\n",
       "      <td>July 26, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112750</th>\n",
       "      <td>Lashonda Patterson</td>\n",
       "      <td>/users/610c02b4f5beef0012e653e1</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>2021-08-05T15:24:36.000Z</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>Clothes</td>\n",
       "      <td>Great clothing</td>\n",
       "      <td>August 05, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93358</th>\n",
       "      <td>Matiya Tate</td>\n",
       "      <td>/users/58d16b650000ff000a840412</td>\n",
       "      <td>US</td>\n",
       "      <td>4 reviews</td>\n",
       "      <td>2021-10-02T13:49:38.000Z</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>Where me and my bf get all our clothes…</td>\n",
       "      <td>Where me and my bf get all our clothes for goi...</td>\n",
       "      <td>October 02, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67067</th>\n",
       "      <td>Jessica Dabrasky</td>\n",
       "      <td>/users/6198fc92ace1180012d8f7a7</td>\n",
       "      <td>US</td>\n",
       "      <td>2 reviews</td>\n",
       "      <td>2021-11-20T13:48:03.000Z</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>Easy and greet</td>\n",
       "      <td>Review text not found</td>\n",
       "      <td>November 20, 2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Reviewer Name                     Profile Link Country  \\\n",
       "23772         Nina Sanchez  /users/63e3c7fcf9dca00015f9f00a      US   \n",
       "99847        Teyana Hudson  /users/614a151187c9cb001243f2cd      US   \n",
       "22028             customer  /users/60bf7b67f78f7400197ca739      US   \n",
       "20917        Savoy Jenkins  /users/62fbc6437fcced0012d098ab      US   \n",
       "118420         Julian Bono  /users/60fed8eed22b22001257a37a      US   \n",
       "67629     Priscilla  Davis  /users/618ad332175ab400159163d6      US   \n",
       "118341     Jizzel tingling  /users/60ff0287f54aad0012de388e      JM   \n",
       "112750  Lashonda Patterson  /users/610c02b4f5beef0012e653e1      US   \n",
       "93358          Matiya Tate  /users/58d16b650000ff000a840412      US   \n",
       "67067     Jessica Dabrasky  /users/6198fc92ace1180012d8f7a7      US   \n",
       "\n",
       "       Review Count               Review Date                  Rating  \\\n",
       "23772      1 review  2023-02-08T18:04:14.000Z  Rated 5 out of 5 stars   \n",
       "99847      1 review  2021-09-21T17:23:31.000Z  Rated 3 out of 5 stars   \n",
       "22028     3 reviews  2023-02-28T20:06:58.000Z  Rated 5 out of 5 stars   \n",
       "20917     2 reviews  2023-03-14T09:03:30.000Z  Rated 5 out of 5 stars   \n",
       "118420     1 review  2021-07-26T15:46:54.000Z  Rated 5 out of 5 stars   \n",
       "67629      1 review  2021-11-09T19:59:47.000Z  Rated 1 out of 5 stars   \n",
       "118341     1 review  2021-07-26T18:44:23.000Z  Rated 5 out of 5 stars   \n",
       "112750     1 review  2021-08-05T15:24:36.000Z  Rated 5 out of 5 stars   \n",
       "93358     4 reviews  2021-10-02T13:49:38.000Z  Rated 5 out of 5 stars   \n",
       "67067     2 reviews  2021-11-20T13:48:03.000Z  Rated 5 out of 5 stars   \n",
       "\n",
       "                                   Review Title  \\\n",
       "23772               The website has great deals   \n",
       "99847      You guys need a better cancellation…   \n",
       "22028                     Easy site to navigate   \n",
       "20917                       I like fashion nova   \n",
       "118420                                    Great   \n",
       "67629                    Hard time checking out   \n",
       "118341                                I love fn   \n",
       "112750                                  Clothes   \n",
       "93358   Where me and my bf get all our clothes…   \n",
       "67067                            Easy and greet   \n",
       "\n",
       "                                              Review Text  Date of Experience  \n",
       "23772                               Review text not found   February 08, 2023  \n",
       "99847        You guys need a better cancellation service.  September 21, 2021  \n",
       "22028                               Review text not found   February 28, 2023  \n",
       "20917   I like fashion nova. Easy to look thru to find...      March 14, 2023  \n",
       "118420                                     Great order!!!       July 26, 2021  \n",
       "67629                               Review text not found   November 09, 2021  \n",
       "118341                                I love me some nova       July 26, 2021  \n",
       "112750                                     Great clothing     August 05, 2021  \n",
       "93358   Where me and my bf get all our clothes for goi...    October 02, 2021  \n",
       "67067                               Review text not found   November 20, 2021  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a64a809c-4174-40ee-bc25-7c4dcf266823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 131980 entries, 0 to 131979\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   Reviewer Name       131980 non-null  object\n",
      " 1   Profile Link        131980 non-null  object\n",
      " 2   Country             131980 non-null  object\n",
      " 3   Review Count        131980 non-null  object\n",
      " 4   Review Date         131980 non-null  object\n",
      " 5   Rating              131980 non-null  object\n",
      " 6   Review Title        131980 non-null  object\n",
      " 7   Review Text         131980 non-null  object\n",
      " 8   Date of Experience  131980 non-null  object\n",
      "dtypes: object(9)\n",
      "memory usage: 9.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6af90c4-1fd0-40bd-973f-9bde51c45c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reviewer Name         0\n",
       "Profile Link          0\n",
       "Country               0\n",
       "Review Count          0\n",
       "Review Date           0\n",
       "Rating                0\n",
       "Review Title          0\n",
       "Review Text           0\n",
       "Date of Experience    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67f5e9dc-fecc-4202-86ea-d000d77477f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviewer Name</th>\n",
       "      <th>Profile Link</th>\n",
       "      <th>Country</th>\n",
       "      <th>Review Count</th>\n",
       "      <th>Review Date</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Date of Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11140</th>\n",
       "      <td>Lashelle Foust</td>\n",
       "      <td>/users/64b81c2e4b86b400126ce1a8</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>2023-07-19T19:23:59.000Z</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>Easy to Order</td>\n",
       "      <td>It was easy! I found exactly what I was lookin...</td>\n",
       "      <td>July 19, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12340</th>\n",
       "      <td>Cherry</td>\n",
       "      <td>/users/5f24ff83ed77f7732dcc5b58</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>2023-07-02T01:08:23.000Z</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>Love it found everything I need</td>\n",
       "      <td>Review text not found</td>\n",
       "      <td>January 07, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34660</th>\n",
       "      <td>Antoinette Moore</td>\n",
       "      <td>/users/6385848b6b646e0012a74a31</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>2022-11-29T06:03:24.000Z</td>\n",
       "      <td>Rated 3 out of 5 stars</td>\n",
       "      <td>The site is very slow on a mobile…</td>\n",
       "      <td>The site is very slow on a mobile device. But ...</td>\n",
       "      <td>November 28, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34800</th>\n",
       "      <td>Julie</td>\n",
       "      <td>/users/59b00ad30000ff000ac4be34</td>\n",
       "      <td>US</td>\n",
       "      <td>2 reviews</td>\n",
       "      <td>2022-11-29T04:52:07.000Z</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>Love clothes</td>\n",
       "      <td>Love clothes. Exactly what I was looking for</td>\n",
       "      <td>November 28, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34960</th>\n",
       "      <td>Demorn Daniels</td>\n",
       "      <td>/users/6385526b6b646e0012a738a6</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>2022-11-29T02:29:33.000Z</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>Great prices and discounts</td>\n",
       "      <td>Review text not found</td>\n",
       "      <td>November 28, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91004</th>\n",
       "      <td>Cat</td>\n",
       "      <td>/users/615daee01c8cd100128e414d</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>2021-10-06T14:12:49.000Z</td>\n",
       "      <td>Rated 1 out of 5 stars</td>\n",
       "      <td>I had to reorder this order because I…</td>\n",
       "      <td>I had to reorder this order because I could no...</td>\n",
       "      <td>October 06, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91005</th>\n",
       "      <td>TONYA HATCH</td>\n",
       "      <td>/users/5bedb33ee35b961c918cb243</td>\n",
       "      <td>US</td>\n",
       "      <td>2 reviews</td>\n",
       "      <td>2021-10-06T14:11:30.000Z</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>First time shopping</td>\n",
       "      <td>Review text not found</td>\n",
       "      <td>October 06, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91006</th>\n",
       "      <td>Kaitlyn Robinson</td>\n",
       "      <td>/users/615dae0cb5371d001231b084</td>\n",
       "      <td>US</td>\n",
       "      <td>1 review</td>\n",
       "      <td>2021-10-06T14:09:17.000Z</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>Review</td>\n",
       "      <td>I love it it fits great and I love the color o...</td>\n",
       "      <td>October 06, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91460</th>\n",
       "      <td>April Marion</td>\n",
       "      <td>/users/57a152db0000ff000a3745e7</td>\n",
       "      <td>US</td>\n",
       "      <td>2 reviews</td>\n",
       "      <td>2021-10-05T18:01:33.000Z</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>This my second time ordering from this…</td>\n",
       "      <td>This my second time ordering from this site I ...</td>\n",
       "      <td>October 05, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109860</th>\n",
       "      <td>Huy Le</td>\n",
       "      <td>/users/611317435b450000128ba3ed</td>\n",
       "      <td>CA</td>\n",
       "      <td>1 review</td>\n",
       "      <td>2021-08-11T00:18:12.000Z</td>\n",
       "      <td>Rated 5 out of 5 stars</td>\n",
       "      <td>Great place to order some nice stuff to…</td>\n",
       "      <td>Great place to order some nice stuff to wear. ...</td>\n",
       "      <td>August 11, 2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Reviewer Name                     Profile Link Country  \\\n",
       "11140     Lashelle Foust  /users/64b81c2e4b86b400126ce1a8      US   \n",
       "12340             Cherry  /users/5f24ff83ed77f7732dcc5b58      US   \n",
       "34660   Antoinette Moore  /users/6385848b6b646e0012a74a31      US   \n",
       "34800              Julie  /users/59b00ad30000ff000ac4be34      US   \n",
       "34960     Demorn Daniels  /users/6385526b6b646e0012a738a6      US   \n",
       "...                  ...                              ...     ...   \n",
       "91004                Cat  /users/615daee01c8cd100128e414d      US   \n",
       "91005        TONYA HATCH  /users/5bedb33ee35b961c918cb243      US   \n",
       "91006   Kaitlyn Robinson  /users/615dae0cb5371d001231b084      US   \n",
       "91460       April Marion  /users/57a152db0000ff000a3745e7      US   \n",
       "109860            Huy Le  /users/611317435b450000128ba3ed      CA   \n",
       "\n",
       "       Review Count               Review Date                  Rating  \\\n",
       "11140      1 review  2023-07-19T19:23:59.000Z  Rated 5 out of 5 stars   \n",
       "12340      1 review  2023-07-02T01:08:23.000Z  Rated 5 out of 5 stars   \n",
       "34660      1 review  2022-11-29T06:03:24.000Z  Rated 3 out of 5 stars   \n",
       "34800     2 reviews  2022-11-29T04:52:07.000Z  Rated 5 out of 5 stars   \n",
       "34960      1 review  2022-11-29T02:29:33.000Z  Rated 5 out of 5 stars   \n",
       "...             ...                       ...                     ...   \n",
       "91004      1 review  2021-10-06T14:12:49.000Z  Rated 1 out of 5 stars   \n",
       "91005     2 reviews  2021-10-06T14:11:30.000Z  Rated 5 out of 5 stars   \n",
       "91006      1 review  2021-10-06T14:09:17.000Z  Rated 5 out of 5 stars   \n",
       "91460     2 reviews  2021-10-05T18:01:33.000Z  Rated 5 out of 5 stars   \n",
       "109860     1 review  2021-08-11T00:18:12.000Z  Rated 5 out of 5 stars   \n",
       "\n",
       "                                    Review Title  \\\n",
       "11140                              Easy to Order   \n",
       "12340            Love it found everything I need   \n",
       "34660         The site is very slow on a mobile…   \n",
       "34800                               Love clothes   \n",
       "34960                 Great prices and discounts   \n",
       "...                                          ...   \n",
       "91004     I had to reorder this order because I…   \n",
       "91005                        First time shopping   \n",
       "91006                                     Review   \n",
       "91460    This my second time ordering from this…   \n",
       "109860  Great place to order some nice stuff to…   \n",
       "\n",
       "                                              Review Text Date of Experience  \n",
       "11140   It was easy! I found exactly what I was lookin...      July 19, 2023  \n",
       "12340                               Review text not found   January 07, 2023  \n",
       "34660   The site is very slow on a mobile device. But ...  November 28, 2022  \n",
       "34800        Love clothes. Exactly what I was looking for  November 28, 2022  \n",
       "34960                               Review text not found  November 28, 2022  \n",
       "...                                                   ...                ...  \n",
       "91004   I had to reorder this order because I could no...   October 06, 2021  \n",
       "91005                               Review text not found   October 06, 2021  \n",
       "91006   I love it it fits great and I love the color o...   October 06, 2021  \n",
       "91460   This my second time ordering from this site I ...   October 05, 2021  \n",
       "109860  Great place to order some nice stuff to wear. ...    August 11, 2021  \n",
       "\n",
       "[66 rows x 9 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b5b08-976a-46bb-be66-d6672527ce65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
